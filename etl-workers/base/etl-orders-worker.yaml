logger:
  level: DEBUG
  format: json
  add_timestamp: true
  static_fields:
    '@service': etl-orders-worker

input:
  kafka:
    addresses:
      - kafka-hub-kafka-bootstrap.kafka-hub.svc.cluster.local:9092
    topics:
      - pg-core.postgres.orders
    consumer_group: "etl-orders-worker"
    start_from_oldest: false
    group:
      session_timeout: 10s
      heartbeat_interval: 3s
      rebalance_timeout: 60s
    batching:
      count: 200
      period: 1s

pipeline:
  processors:
    - log:
        level: DEBUG
        message: "received messages"
        fields_mapping: |
          root.data = this

    - mapping: |
        root = this.after

output:
  label: clickhouse
  sql_raw:
    driver: "clickhouse"
    dsn: "clickhouse://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}"
    unsafe_dynamic_query: true
    batching:
      count: 200
      period: 1s
    query: >
      INSERT INTO trx_orders (  id, created_at, completed_at, customer_id, customer_name, order_number, order_state, order_type, total_price )
      SETTINGS async_insert=1, wait_for_async_insert=1, wait_for_async_insert_timeout=120 
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);
    args_mapping: |
      root = [
        this.id, 
        this.created_at.ts_parse("2006-01-02T15:04:05.000000Z").ts_format("2006-01-02 15:04:05.000000000"),
        this.completed_at.ts_parse("2006-01-02T15:04:05.000000Z").ts_format("2006-01-02 15:04:05.000000000"),
        this.customer_id,
        this.customer_name,
        this.order_number,
        this.order_state,
        this.order_type,
        this.total_price
      ]
